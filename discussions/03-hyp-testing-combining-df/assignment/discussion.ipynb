{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"discussion.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80 - Discussion 03\n",
    "\n",
    "### Due Date: Saturday October 15, 11:59 PM\n",
    "\n",
    "**Discussions will be due by the end of the day on Saturday**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for formatting purposes\n",
    "def multi_table(table_list):\n",
    "    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n",
    "    '''\n",
    "    return HTML(\n",
    "        '<table><tr style=\"background-color:white;\">' + \n",
    "        ''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list]) +\n",
    "        '</tr></table>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discussion import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Hypothesis Testing & Combining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover examples of two key types of hypothesis testing\n",
    "* Comparing two categorical distributions\n",
    "* Comparing sub-group statistic to a population parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to follow to solve a hypothesis testing problem\n",
    "1. Form the Null and Alternate hypothesis\n",
    "2. Define the test statistic\n",
    "3. Calculate observed/sample test statistic (form an intuition of how big/small/extreme it is)\n",
    "4. Simulate one instance of test statistic under null hypothesis. Simulate the whole null distribution\n",
    "5. Calculate p-value based on null distribution and observed test statistic\n",
    "6. Plot the null distribution, observed statistic and validate the p-value\n",
    "7. Provide conclusion as to whether you reject or fail-to-reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Snapchat users similar in age distribution compared to overall social media users?\n",
    "Based on the (fake) survey collected from 1000 Snapchat users, we want to understand if the age distribution of these two user groups are significantly different?\n",
    "- Note that the survey data below is already aggregated, which can be used directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = pd.DataFrame([['GenZ', 0.64, 0.48],\n",
    "                    ['Millennials', 0.24, 0.36],\n",
    "                    ['GenX', 0.08, 0.10],\n",
    "                    ['Boomers', 0.04, 0.06]],\n",
    "                   columns=['Age Group', 'Snapchat', 'Social Media']).set_index('Age Group')\n",
    "\n",
    "age_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Form the Null and Alternate Hypotheses\n",
    "Null hypothesis:\n",
    "\n",
    "Alternate hypothesis: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-2: Define the test-statistic\n",
    "\n",
    "A common test statistic used while comparing two categorical distributions is TVD (Total Variation Distance)\n",
    "\n",
    "**TVD:** Sum of absolute differences between corresponding values of two distribution, divided by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3: Calculate observed/sample test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df.plot(kind='barh', title='Age Group Distribution of Snapchat vs Social Media');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_tvd = np.sum(np.abs(age_df['Snapchat'] - age_df['Social Media'])) / 2\n",
    "observed_tvd\n",
    "\n",
    "# Think - What would be the observed TVD if the two groups are exactly similar?\n",
    "# Further - What does this imply about the TVD value as the difference increases/decreases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-4 Simulate the test-statistic under null hypothesis. And simulate the null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate one instance - Keep null hypothesis in mind\n",
    "# There is no difference between Snapchat age groups wrt. overall age groups.\n",
    "# So, a sample under null hypothesis should like an 'overall social media' distribution.\n",
    "np.random.multinomial(1000, age_df['Social Media']) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simulate N times - Larger the N, smoother will be the Null distribution\n",
    "# Traditional way - Use a for loop and store the null test statistic\n",
    "# Faster way - Utilize numpy functionalities\n",
    "N = 5000\n",
    "np.random.multinomial(1000, age_df['Social Media'], size=N) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_dists = np.random.multinomial(1000, age_df['Social Media'], size=N) / 1000\n",
    "\n",
    "# Power of broadcasting: Can take difference between 2D array and 1D array \n",
    "# by broadcasting 1D array to 2D array\n",
    "null_tvds = np.sum(np.abs(null_dists - age_df['Social Media'].to_numpy()), axis=1) / 2\n",
    "null_tvds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-5: Calculate p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_val = np.mean(null_tvds >= observed_tvd)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-6: Visualize and validate the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.Series(null_tvds).plot(kind='hist', \n",
    "                     density=True,\n",
    "                     ec='w',\n",
    "                     title='Simulated Null TVDs & Observed TVD');\n",
    "plt.axvline(x=observed_tvd, color='red', linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Null Hypothesis - \n",
    "\n",
    "#  Think - What happens to your belief about the null hypothesis if we assume the distribution is taken \n",
    "#  from only 10 or 100 users?\n",
    "#  Would you believe the null hypothesis more or believe it less??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-7: Provide conclusion\n",
    "\n",
    "Conclusion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "A study from a competitive programming test shows a lower average score for students who used 'Go' programming language compared to others. Test whether this lower avg. score of 'Go' programming language is due to chance alone.\n",
    "\n",
    "- The function should take a DataFrame like `prog_df`, number of null test statistic simulations `N`, \n",
    "- It should return a list containing a) observed test statistic, b) and the p-value of the hypothesis test. Note that the function should work for any `prog_df` having same column names, and the same unique values in `language` column.\n",
    "\n",
    "Hint: Don't forget to utilize `numpy` functionality to eliminate the `for` loop. Check lecture 06 final example for a very similar problem and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_df = pd.read_csv(os.path.join('data','prog_df.csv'))\n",
    "grouped_df = prog_df.groupby('language')['score'].agg(['mean', 'count'])\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the test statistic?\n",
    "# What's the observed test statistic?\n",
    "# How to simulate the null hypothesis?\n",
    "# p-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "\n",
    "prog_df = pd.read_csv(os.path.join('data','prog_df.csv'))\n",
    "q1_out = hyp_test_lower_avg(prog_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames \n",
    "\n",
    "* [`concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "* [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)\n",
    "* [`join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `concat()`\n",
    "\n",
    "* Used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).\n",
    "    * Useful if we have two or more data sets containing the same columns but different rows of data.\n",
    "    * We can also concat the columns from one `Dataframe` to those of another `Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left dataframe\n",
    "left = pd.DataFrame({\n",
    "   'id':[1,2,3,4,5],\n",
    "   'Name': ['Aaron', 'Marina', 'Justin', 'Janine', 'Ilya'],\n",
    "   'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n",
    "\n",
    "# right dataframe\n",
    "right = pd.DataFrame(\n",
    "   {'id':[1,2,3,4,5],\n",
    "   'Name': ['Enrique', 'Parker', 'Erik', 'Allston', 'Betty'],\n",
    "   'subject_id':['sub2','sub4','sub3','sub6','sub5']})\n",
    "\n",
    "multi_table([left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'left' below 'right'\n",
    "pd.concat([right, left])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to keep track of the names dataframes after concat, use 'keys'\n",
    "pd.concat([right, left], keys=['right', 'left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'left' to the side of 'right'\n",
    "pd.concat([right, left], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `merge()`\n",
    "\n",
    "* Used to combine two (or more) dataframes on the basis of **values of common columns** (indices can also be used, use `left_index=True` and/or `right_index=True`).\n",
    "    * If we are joining columns on columns, the DataFrame indexes will be ignored. \n",
    "    * If we are joining indexes on indexes or indexes on a column or columns, the index will be passed on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`on`**: column or index names to join on. \n",
    "    * These must be found in both DataFrames. \n",
    "    * If `on` is `None` and not merging on indexes, then this defaults to the intersection of the columns in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table([left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge left and right tables on 'id' column\n",
    "on_id = pd.merge(left,right,on='id')\n",
    "\n",
    "# how many rows, how many columns?\n",
    "multi_table([left, right, on_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge left and right tables on 'id' and 'subject_id' columns\n",
    "on_id_subject = pd.merge(left,right,on=['id', 'subject_id'])\n",
    "\n",
    "# how many rows, how many columns, what are the indices?\n",
    "multi_table([left, right, on_id_subject])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`how`**: specifies how to determine which keys are to be included in the resulting table. \n",
    "    * If a key (column name) combination does not appear in either the left or the right tables, the values in the joined table will be `np.NaN`.\n",
    "    * Defaults to `inner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = {\n",
    "        'subject_id': ['1', '2', '3', '4', '5'],\n",
    "        'first_name': ['Manny', 'Will', 'Hunter', 'Ian', 'Eric'], \n",
    "        'last_name': ['Machado', 'Myers', 'Renfroe', 'Kinsler', 'Hosmer']}\n",
    "df_a = pd.DataFrame(data_a, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "\n",
    "data_b = {\n",
    "        'subject_id': ['4', '5', '6', '7', '8'],\n",
    "        'first_name': ['Cody', 'Justin', 'Corey', 'Clayton', 'Kenley'], \n",
    "        'last_name': ['Bellinger', 'Turner', 'Seager', 'Kershaw', 'Jansen']}\n",
    "df_b = pd.DataFrame(data_b, columns = ['subject_id', 'first_name', 'last_name'])\n",
    "\n",
    "multi_table([df_a, df_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Merge Method  | Description                  |\n",
    "| :-------      | :---------------------------:| \n",
    "| `left`        | Use keys from left object    | \n",
    "| `right`       | Use keys from right object   | \n",
    "| `outer`       | Use union of keys            |\n",
    "| `inner`       | Use intersection of keys     | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the output below, what 'how' argument was passed into pd.merge?\n",
    "how_list = ['outer', 'inner', 'right', 'left']\n",
    "\n",
    "merge_method = np.random.choice(how_list)\n",
    "\n",
    "pd.merge(df_a, df_b, on='subject_id', how=merge_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check!\n",
    "merge_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `join()`\n",
    "\n",
    "* Used to merge two dataframes on the basis of the index; instead of using `merge()` with the option `left_index=True`, we can use `join()`.\n",
    "    * Join operation honors the object on which it is called: `a.join(b)` $ \\neq $ `b.join(a)`.\n",
    "\n",
    "Facts about `merge()` vs `join()`:\n",
    "\n",
    "* `merge()` is the underlying function used for all merge/join behavior\n",
    "* `join()` is basically a specific behavior of `merge()` (left join using indices)\n",
    "* For all practical purposes, `merge()` is usually used. While speaking, in general, merging and joining mean the same thing - combining DataFrames/tables based on common columns or indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/join_types.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Inner Join** – only keep rows where the merge “on” value exists in both the left and right dataframes.\n",
    "2. **Left Outer** – keep every row in the left dataframe.\n",
    "    * Where there are missing values of the “on” variable in the right dataframe, add `np.NaN` values in the result.\n",
    "3. **Right Join** – keep every row in the right dataframe. \n",
    "    * Where there are missing values of the “on” variable in the left column, add `np.NaN` values in the result.\n",
    "4. **Outer Join** – returns all the rows from the left dataframe, all the rows from the right dataframe, and matches up rows where possible, with `NaNs` elsewhere.\n",
    "\n",
    "We'll start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')\n",
    "df2 = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')\n",
    "\n",
    "joined = df1.join(df2, lsuffix='_l', rsuffix='_r')\n",
    "\n",
    "multi_table([df1, df2, joined])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try something a bit more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_data = {\n",
    "    'Year' : [2014, 2014, 2014, 2014, 2014],\n",
    "    'Week' : ['A', 'B', 'B', 'C', 'D'],\n",
    "    'Color' : ['Red', 'Red', 'Black', 'Red', 'Green'],\n",
    "    'Val' : [50, 60, 70, 10, 20]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(df1_data).set_index('Week')\n",
    "\n",
    "df2_data = {\n",
    "    'Year' : [2014, 2014, 2014, 2014, 2014],\n",
    "    'Week' : ['A', 'B', 'C', 'C', 'D'],\n",
    "    'Color' : ['Black', 'Black', 'Green', 'Red', 'Red'],\n",
    "    'Score' : [30, 100, 50, 20, 40]\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(df2_data).set_index('Week')\n",
    "\n",
    "multi_table([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows, how many columns?\n",
    "df1.join(df2, lsuffix='_l', rsuffix = '_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will this be any different?\n",
    "df2.join(df1, lsuffix='_l', rsuffix = '_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Interview Question\n",
    "How many rows will you get if you perform:\n",
    "- a) `df1` **left join** `df2` on `'letter'` ?\n",
    "- b) `df1` **inner join** `df2` on `'letter'` ?\n",
    "- c) `df1` **right join** `df2` on `'letter'` ?\n",
    "\n",
    "Answer the question without using python code.\n",
    "Can you write how the final merged/joined table will look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\n",
    "    'letter' : [1, 1, 2, 3, 4, 4],\n",
    "    'alphabet' : ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'letter' : [1, 2, 4, 4, 4],\n",
    "    'alphabet' : ['G', 'H', 'I', 'J', 'K']\n",
    "})\n",
    "\n",
    "multi_table([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "You are given two seperate dataframes: `mlb_2017` and `mlb_2018`. Both dataframes contain statistics for the 2017 and 2018 baseball seasons respectively. Your job is two combine these two dataframes into one using the following guidelines:\n",
    "\n",
    "* The dataframe you return should be indexed by team name (`Tm`).\n",
    "* The dataframe you return should include all columns from both `mlb_2017` and `mlb_2018`.\n",
    "* Use the suffixes `_2017` and `_2018` to differentiate between statistics from both seasons.\n",
    "\n",
    "Create a function `combined_seasons` that returns, as a tuple, the following:\n",
    "\n",
    "* The combined dataframe described above.\n",
    "* The team with most homeruns (`HR`) bewteen the 2017 and 2018 seasons combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the following .txt files\n",
    "mlb_2017 = pd.read_csv(os.path.join('data','mlb_2017.txt'))\n",
    "mlb_2018 = pd.read_csv(os.path.join('data','mlb_2018.txt'))\n",
    "\n",
    "multi_table([mlb_2017.head(), mlb_2018.head()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "mlb_2017 = pd.read_csv(os.path.join('data','mlb_2017.txt'))\n",
    "mlb_2018 = pd.read_csv(os.path.join('data','mlb_2018.txt'))\n",
    "q2_out = combined_seasons(mlb_2017, mlb_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Using the same two dataframes, `mlb_2017` and `mlb_2018`, create a function `seasonal_average` that combines them and takes the mean of each column for each team. \n",
    "\n",
    "* The dataframe you return should be indexed by team name (`Tm`).\n",
    "* Each column should contain the average value between the *2017* and *2018* seasons for the given statistic for each team.\n",
    "    * For example, the `HR` column should contain the average value for `HR` for each team between the *2017* and *2018* seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "mlb_2017 = pd.read_csv(os.path.join('data','mlb_2017.txt'))\n",
    "mlb_2018 = pd.read_csv(os.path.join('data','mlb_2018.txt'))\n",
    "q3_out = seasonal_average(mlb_2017, mlb_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit your `.py` file to Gradescope. Note that you only need to submit the `.py` file; this notebook should not be uploaded. Make sure that all of your work is in the `.py` file by running the doctests: `python -m doctest discussion.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m doctest discussion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q1_out, list)\nTrue",
         "failure_message": "doctest: type",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(q1_out[0], 76.96903195339905, atol=0.01)\nTrue",
         "failure_message": "doctest: observed test statistic",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0.08 <= q1_out[1] <= 0.20\nTrue",
         "failure_message": "doctest: p-value",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q2_out[0].shape == (30, 56)\nTrue",
         "failure_message": "doctest: shape",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q2_out[1] in q2_out[0].index\nTrue",
         "failure_message": "doctest: index",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([(('_2017' in x) or ('_2018' in x)) for x in q2_out[0]])\nTrue",
         "failure_message": "doctest: columns",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> q2_out[1] == 'NYY'\nTrue",
         "failure_message": "doctest: most homruns incorrect",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3_out.shape == (30, 28)\nTrue",
         "failure_message": "doctest: shape",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q3_out.index.nunique() == 30\nTrue",
         "failure_message": "doctest: index",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q3_out.loc['MIN']['HR'] == 186\nTrue",
         "failure_message": "doctest: MIN HR",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q3_out.loc['OAK'].max() == 6190.5\nTrue",
         "failure_message": "doctest: OAK row",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
